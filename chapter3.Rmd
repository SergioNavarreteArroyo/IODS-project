# Logistic Regression

## 1 | Load input data

---
```{r}
library(readr)
alc <- read_csv("C:\\LocalData\\navarret\\IODS\\IODS-project\\data\\create_alc.csv")
str(alc)
dim(alc)
head(alc)
```

## 2 | Relationships between variables

The purpose of the analysis now is to study the relationships between the variables of our data. 
We will explore the relationship between high/low alcohol consumption and the following 4 variables: health, absences, G1 and G3. To do it, we will create multiple boxplots. 
Our hypothesis are the following: 
* * Lower alcohol use is related with a better health.
* Lower alcohol use is related with a smaller rate of school absence. 
* Lower alcohol use is related with better grades (G1 and G3 variables)

We will start by looking at the distribution of the variables, as well as the correlation between them:

```{r}
library(ggplot2)
library(GGally)
alc2 <- data.frame(alc$health, alc$absences, alc$G1, alc$G3, alc$high_use)
p <- ggpairs(alc2,mapping = aes(col = alc.high_use, alpha = 0.3), lower = list(combo = wrap("facethist", bins = 20)))
print(p)
```

Now we can look at the boxplots in more detail to see the effect of the consumption of alcohol specfically:

```{r}
library(ggpubr)

p1 <- ggboxplot(alc, x = "high_use", y = "health",
                 color = "high_use", palette = "jco")
p2 <- ggboxplot(alc, x = "high_use", y = "absences",
                 color = "high_use", palette = "jco")
p3 <- ggboxplot(alc, x = "high_use", y = "G1",
                 color = "high_use", palette = "jco")
p4 <- ggboxplot(alc, x = "high_use", y = "G3",
                 color = "high_use", palette = "jco")

ggarrange(p1, p2, p3, p4 + rremove("x.text"), 
          ncol = 2, nrow = 2)

```

And now lets see some cross-tables: 

```{r}
library(dplyr)
alc %>% group_by(high_use) %>% summarise(count = n(), mean_health = mean(health), mean_absences = mean(absences), mean_G1 = mean(G1), mean_G3 = mean(G3)) 
```

By looking at all these graphs and tables, we can see that two out of our three hypothesis are correct. We can see that those subjects with lower consumption of alcohol exhibit higher absences and lower grades than subjects with higher alcohol consumpion. 

## 3 | Logistic regression

Now we will use logistic regression to statistically explore the relationship between my chosen variables and the binary high/low alcohol consumption variable as the target variable.
We will see the summary and coeficients of the model. 
By looking at this data, we can say that the only variable that was significantly associated with the alcohol consumption was the absences. Then, only our second hypothesis was confirmed. 

```{r}
# find the model with glm()
m <- glm(high_use ~ health + absences + G1 + G3, data = alc, family = "binomial")
# print out a summary of the model
summary(m)
# print out the coefficients of the model
coef(m)
```

And now, we will look at the odds ratios and intervals of confidence. Regarding the absences, we can see that those subjects high higher alcohol consumption had a larger number of absences from school (ODDS larger than 1), although the effect is not really high.
We can only trust for the absences, since the confidence intervals does not include 1. If it includes 1, we cannot think that the OR is statistically significant. 

```{r}
# compute odds ratios (OR)
OR <- coef(m) %>% exp
# compute confidence intervals (CI)
CI <- confint(m) %>% exp
# print out the odds ratios with their confidence intervals
cbind(OR, CI)
```
## 4 | Predicitve power of the model

### Predictive power

Refining the model: culling variables that showed no statistical significance
Health, G1 and G3 will be dropped, absences remain. 

```{r}
m <- glm(high_use ~ absences, data = alc, family = "binomial")

```

Here, we take the model from before, and based on it calculate the probability that a certain subject is considered as a higher alcohol drinker.    
 

```{r}
 
alc$predicted_probabilities <- predict(m, type = "response")

alc <- mutate(alc,prediction  = predicted_probabilities > 0.5)  

```

## Comparison of actual values (High use or not) and predicted values. 


```{r}

x<-alc %>% select(absences, high_use, predicted_probabilities, prediction) 
x<-table(high_use = x$high_use, prediction = x$prediction)
y<-x
y<-(y/370)*100
print(x)
print (y)
```
### Penalty function definition

```{r, echo=T}

# define a loss function (mean prediction error)
loss_func <- function(class, prob) {
  n_wrong <- abs(class - prob) > 0.5 
  mean(n_wrong)
}

```

### Cross validation

```{r, results=T}


# K-fold cross-validation
library(boot)
cv <- cv.glm(data = alc, cost = loss_func, glmfit = m, K = 10)

# average number of wrong predictions in the cross validation
cv$delta[1]

```
The predictors that are looking at now at have a bit higher mean prediction error, but this is not super high. 
